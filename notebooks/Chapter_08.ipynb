{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "**Chapter 08**\n",
                "# **Introduction to deep learning for computer vision**\n",
                "---\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
                    ]
                }
            ],
            "source": [
                "# System Libraries\n",
                "import importlib, sys, os\n",
                "import numpy as np\n",
                "\n",
                "# TensorFlow Libraries\n",
                "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
                "from tensorflow import keras\n",
                "import tensorflow as tf\n",
                "\n",
                "# User Libraries\n",
                "sys.path.append(\"../\")\n",
                "from modules import common, chapter_01, chapter_08\n",
                "\n",
                "# Reload Libraries\n",
                "importlib.reload(chapter_01)\n",
                "importlib.reload(chapter_08)\n",
                "importlib.reload(common)\n",
                "\n",
                "# Check GPU\n",
                "print(tf.config.list_physical_devices())\n",
                "\n",
                "# Module variables\n",
                "batch_size = 512\n",
                "epochs = 10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Convolution theory**\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### <ins />**Convolution layers**\n",
                "  - Dense layers learn patterns of global input feature map (thus not used in object detection)\n",
                "  - Conv layers learn patterns of local input patch\n",
                "  - Convolution classification network has two parts:\n",
                "     - Convolution base: Conv2D + Maxpooling\n",
                "     - Logit: Dense\n",
                "  - Convolution base:\n",
                "    - Lower layers extract specific feature maps\n",
                "    - Upper layers extract highly generic feature maps\n",
                "  - Spatial hierarchical patterns\n",
                "    - Upper layers learn generic patterns\n",
                "    - Lower layers learn specific patterns\n",
                "  - Translation invariant patterns\n",
                "    - After learning a pattern once, conv layer can infer it anywhere in the image\n",
                "  - Dense layer is also called fully connected layer\n",
                "\n",
                "### <ins />**Feature map**\n",
                "  - Height, Width, Channels (H, W, D)\n",
                "  - Feature map size: HxW\n",
                "  - Feature map depth: D\n",
                "  - For larger images, we increase number of layers:\n",
                "    - To increase feature map depth\n",
                "    - To decrease feature map size\n",
                "    - To increase model capacity\n",
                "  - In a convnet\n",
                "    - Feature map depth increases with each layer\n",
                "    - Feature map size decreases with each layer\n",
                "  \n",
                "### <ins />**Input feature map (3D)**\n",
                "  - 28x28x1 matrix input image\n",
                "\n",
                "### <ins />**Filter**\n",
                "  - Depth of output feature map\n",
                "  - 32, 64, etc.\n",
                "\n",
                "### <ins />**Kernel (2D)**\n",
                "  - 3x3 matrix aka convolution kernel / structuring element\n",
                "  - Same kernel is used with all input patches\n",
                "\n",
                "### <ins />**Input patch (2D)**\n",
                "  - 3x3 matrix patch of input feature map\n",
                "\n",
                "### <ins />**Response map (1D)**\n",
                "  - 3x1 tensorproduct of kernel and input patch\n",
                "\n",
                "### <ins />**Output feature map (3D)**\n",
                "  - Combine all response maps into matrix\n",
                "  \n",
                "### <ins />**Convolution operation (sliding window)**\n",
                "  1. Slide the kernel on input feature map\n",
                "  2. Extract input patch\n",
                "  3. Tensor product the kernel and input patch to get response map vector\n",
                "  4. Combine all response map vectors (one per patch) to get output feature map\n",
                "  5. Output map size is less than input map size due to convolution border effect and stride\n",
                "\n",
                "### <ins />**Padding**\n",
                "  - To counter border effect i.e. in order to have output map size equals input map size\n",
                "  - Valid padding\n",
                "    - No padding applied (default)\n",
                "    - Output map size < Input map size\n",
                "  - Same padding\n",
                "    - Padding applied\n",
                "    - Output map size = Input map size\n",
                "    - Padding size depends on kernel size, not on input patch size\n",
                "    - Padding for 3x3 kernel:\n",
                "      - 1 row on top. 1 row on bottom\n",
                "      - 1 column on right. 1 column on left\n",
                "    - Padding for 5x5 kernel:\n",
                "      - 2 rows on top. 2 rows on bottom\n",
                "      - 2 columns on right. 2 columns on left\n",
                "\n",
                "### <ins />**Stride**\n",
                "  - **Used when spatial location information is important (segmentation, detection)**\n",
                "  - Distance between two consecutive convolution windows\n",
                "  - Downsampling mechanism\n",
                "  - Stride=1 (default)\n",
                "\n",
                "### <ins />**Maxpooling**\n",
                "  - **Used when spatial location information is not important (classification)**\n",
                "  - Max patch: 2x2 matrix of max value of input patch (no information of location)\n",
                "  - Output: Tensorproduct of max patch and input patch\n",
                "  - It  detroys location information\n",
                "  - Downsampling mechanism\n",
                "\n",
                "### <ins />**Convolution vs Maxpooling vs Avgpooling**\n",
                "  - Maxpooling:\n",
                "    - Kernel=2x2 (pool size)\n",
                "    - Stride=2\n",
                "    - **Kernel uses max value of input patch**\n",
                "    - Feature map size is reduced by **size / 2**\n",
                "  - Avgpooling:\n",
                "    - Kernel=2x2 (pool size)\n",
                "    - Stride=2\n",
                "    - **Kernel uses average value of input patch**\n",
                "    - Feature map size is reduced by **size / 2**\n",
                "  - Convolution:\n",
                "    - Kernel=3x3 (kernel size)\n",
                "    - Stride=1\n",
                "    - **Kernel uses fixed value irrespective of input patch**\n",
                "    - Feature map size is reduced by **size - 2**\n",
                "\n",
                "### <ins />**Why downsample (conv, stride, maxpooling, avgpooling)**\n",
                "  - Reduce number of coefficients and overfit\n",
                "    - Without maxpooling = 61952 coefficients \n",
                "    - With maxpooling = 1152 coefficients  \n",
                "  - Increase channel-to-height/width ratio\n",
                "    - Without maxpooling = 24x24x64\n",
                "    - With maxpooling = 11x11x64\n",
                "\n",
                "### <ins />**Data augmentation**\n",
                "   - Remix the already available information\n",
                "   - No new information created\n",
                "   - No effect during inference\n",
                "   - Used along with dropout\n",
                "\n",
                "\n",
                "### <ins />**Transfer learning**\n",
                "  - Models trained on large dataset (different classes) serve as generic model of visual world\n",
                "  - E.g. Model trained on ImageNet (animals) used as base model for bottle detection\n",
                "  - Deep learning with convet is effective for small dataset due to  transfer learning\n",
                "  - Types of transfer learning: \n",
                "    - Feature extraction\n",
                "    - Fine tuning\n",
                "\n",
                "### <ins />**Feature extraction**\n",
                "  - Take convolution base of another model and add a new fully connected (dense) layer\n",
                "  - Using dense layer from another model **should be avoided**\n",
                "  - New dataset has similar classes:\n",
                "    - Use all layers of convolution base except dense layer\n",
                "  - New dataset has different classes:\n",
                "    - Use upper layers of convolution base\n",
                "  - Types of feature extraction:\n",
                "    - Conv base not part of training:\n",
                "      - Use conv base to extract features once before training\n",
                "      - Feed conv base output to dense classifier during training\n",
                "      - Conv base is not part of training\n",
                "      - Augmentation layer not possible\n",
                "      - Faster\n",
                "    - Conv base part of training\n",
                "      - Train end-to-end conv base along with dense classifier\n",
                "      - Weights of conv base should be frozen\n",
                "      - Augmentation layer possible\n",
                "      - Slower\n",
                "\n",
                "### <ins />**Fine tuning**\n",
                "  - In fine tuning, only few upper layers are frozen (containing generic information)\n",
                "  - In feature extractions, all layers are frozen"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Explore maxpooling and padding**\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Case-1: No padding. No maxpooling**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = keras.Input(shape=(28, 28, 1))\n",
                "x = keras.layers.Conv2D(filters=32, kernel_size=3, activation=tf.nn.relu)(inputs)\n",
                "x = keras.layers.Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.Flatten()(x)\n",
                "outputs = keras.layers.Dense(units=10, activation=tf.nn.softmax)(x)\n",
                "keras.Model(inputs=inputs, outputs=outputs).summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Case-2: With padding. No maxpooling**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = keras.Input(shape=(28, 28, 1))\n",
                "x = keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=tf.nn.relu)(inputs)\n",
                "x = keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=tf.nn.relu)(x)\n",
                "x = keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=tf.nn.relu)(x)\n",
                "x = keras.layers.Flatten()(x)\n",
                "outputs = keras.layers.Dense(units=10, activation=tf.nn.softmax)(x)\n",
                "keras.Model(inputs=inputs, outputs=outputs).summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Case-3: No padding. With maxpooling**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = keras.Input(shape=(28, 28, 1))\n",
                "x = keras.layers.Conv2D(filters=32, kernel_size=3, activation=tf.nn.relu)(inputs)\n",
                "x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.Flatten()(x)\n",
                "outputs = keras.layers.Dense(units=10, activation=tf.nn.softmax)(x)\n",
                "keras.Model(inputs=inputs, outputs=outputs).summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Case-4: Model with padding, with maxpooling**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = keras.Input(shape=(28, 28, 1))\n",
                "x = keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=tf.nn.relu)(inputs)\n",
                "x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=tf.nn.relu)(x)\n",
                "x = keras.layers.Flatten()(x)\n",
                "outputs = keras.layers.Dense(units=10, activation=tf.nn.softmax)(x)\n",
                "keras.Model(inputs=inputs, outputs=outputs).summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Tensorflow Dataset API**\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Efficient input batch pipeline\n",
                "- Asynchronous data prefetching (fetch new batch while previous batch is being handled by model)\n",
                "- normal vs uniform distributions?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = [x for x in range(0, 20)]\n",
                "dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
                "# Shuffle\n",
                "dataset = dataset.shuffle(len(dataset))\n",
                "# Batch\n",
                "dataset = dataset.batch(5)\n",
                "for batch in dataset:\n",
                "    print(\"Original: \", batch)\n",
                "# Reduce\n",
                "reduced = dataset.reduce(initial_state=0, reduce_func=lambda x, y: x + y)\n",
                "print(\"Reduced: \", reduced)\n",
                "# Map\n",
                "dataset = dataset.map(lambda x: x * 0)\n",
                "for batch in dataset:\n",
                "    print(\"Mapped: \", batch)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Mnist convolution network**\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Notes**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Between conv and dense layers, always use flatten and dropout layers\n",
                "    - Conv layer\n",
                "    - Flatten\n",
                "    - Dropout\n",
                "    - Dense layer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Dataset**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
                "x_train = x_train.reshape(60000, 28, 28, 1)\n",
                "x_test = x_test.reshape(10000, 28, 28, 1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Model**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = keras.Input(shape=(28, 28, 1))\n",
                "x = keras.layers.Rescaling(1.0 / 255)(inputs)\n",
                "x = keras.layers.Conv2D(filters=32, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPool2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPool2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPool2D(pool_size=2)(x)\n",
                "x = keras.layers.Flatten()(x)\n",
                "x = keras.layers.Dropout(rate=0.5)(x)\n",
                "outputs = keras.layers.Dense(units=10, activation=tf.nn.softmax)(x)\n",
                "model = keras.Model(inputs=inputs, outputs=outputs)\n",
                "model = chapter_01.compile(model=model)\n",
                "model = keras.Model(inputs=inputs, outputs=outputs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Train**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = chapter_01.compile(model=model)\n",
                "history = chapter_01.train(x_train, y_train, model, batch_size=batch_size, callbacks=common.callbacks(), epochs=epochs)\n",
                "chapter_01.evaluate(x_test, y_test, model)\n",
                "common.plot(data=[history], labels=[\"00_mnist_convnet\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Kaggle cats-vs-dog dataset**\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Dataset**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_base_dir, train_dataset, val_dataset, test_dataset = chapter_08.dataset_batches()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Augmentation**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "augmentation = keras.Sequential(\n",
                "    [\n",
                "        keras.layers.RandomFlip(\"horizontal\"),\n",
                "        keras.layers.RandomRotation(0.1),\n",
                "        keras.layers.RandomZoom(0.2),\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Training from scratch without augmentation**\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Model**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = keras.Input(shape=(180, 180, 3))\n",
                "x = keras.layers.Rescaling(1.0 / 255)(inputs)\n",
                "x = keras.layers.Conv2D(filters=32, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPool2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPool2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.Conv2D(filters=256, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPool2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=256, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.Flatten()(x)\n",
                "x = keras.layers.Dropout(rate=0.5)(x)\n",
                "outputs = keras.layers.Dense(units=1, activation=tf.nn.sigmoid)(x)\n",
                "model = keras.Model(inputs=inputs, outputs=outputs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Train**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = \"01_train_scratch.keras\"\n",
                "chapter_08.train_batch(train_dataset, val_dataset, test_dataset, model, model_path, epochs, batch_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Training from scratch with augmentation**\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Model**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = keras.Input(shape=(180, 180, 3))\n",
                "x = augmentation(inputs)\n",
                "x = keras.layers.Rescaling(1.0 / 255)(x)\n",
                "x = keras.layers.Conv2D(filters=32, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPool2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPool2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.Conv2D(filters=256, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.MaxPool2D(pool_size=2)(x)\n",
                "x = keras.layers.Conv2D(filters=256, kernel_size=3, activation=tf.nn.relu)(x)\n",
                "x = keras.layers.Flatten()(x)\n",
                "x = keras.layers.Dropout(rate=0.5)(x)\n",
                "outputs = keras.layers.Dense(units=1, activation=tf.nn.sigmoid)(x)\n",
                "model = keras.Model(inputs=inputs, outputs=outputs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Train**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = \"02_train_scratch_augmentation.keras\"\n",
                "chapter_08.train_batch(train_dataset, val_dataset, test_dataset, model, model_path, epochs, batch_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Training using feature extraction without augmentation**\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Convolution base**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "is_executing": true
            },
            "outputs": [],
            "source": [
                "conv_base = keras.applications.vgg16.VGG16(\n",
                "    weights=\"imagenet\",\n",
                "    include_top=False,\n",
                "    input_shape=(180, 180, 3),\n",
                ")\n",
                "print(\"Weights: \", len(conv_base.trainable_weights))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Features**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_features(dataset):\n",
                "    features, labels = [], []\n",
                "    for images_batch, labels_batch in dataset:\n",
                "        images_batch = keras.applications.vgg16.preprocess_input(x=images_batch)\n",
                "        features_batch = conv_base.predict(x=images_batch, verbose=False)\n",
                "        features.append(features_batch), labels.append(labels_batch)\n",
                "    return np.concatenate(features), np.concatenate(labels)\n",
                "\n",
                "\n",
                "x_train, y_train = extract_features(train_dataset)\n",
                "x_val, y_val = extract_features(val_dataset)\n",
                "x_test, y_test = extract_features(test_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Model**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = keras.Input(shape=(5, 5, 512))\n",
                "x = keras.layers.Flatten()(inputs)\n",
                "x = keras.layers.Dense(units=256, activation=None)(x)\n",
                "x = keras.layers.Dropout(rate=0.5)(x)\n",
                "outputs = keras.layers.Dense(units=1, activation=tf.nn.sigmoid)(x)\n",
                "model = keras.Model(inputs=inputs, outputs=outputs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Train**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = \"03_train_feature_extraction.keras\"\n",
                "chapter_08.train(x_train, y_train, x_val, y_val, x_test, y_test, model, model_path, epochs, batch_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Training using feature extraction with augmentation**\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Convolution base**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "conv_base = keras.applications.vgg16.VGG16(\n",
                "    weights=\"imagenet\",\n",
                "    include_top=False,\n",
                ")\n",
                "# Freeze all layers (dont overwrite conv base weights)\n",
                "conv_base.trainable = False\n",
                "print(\"Weights: \", len(conv_base.trainable_weights))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Model**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_model(conv_base):\n",
                "    inputs = keras.Input(shape=(180, 180, 3))\n",
                "    x = augmentation(inputs)\n",
                "    x = keras.applications.vgg16.preprocess_input(x)\n",
                "    x = conv_base(x)\n",
                "    x = keras.layers.Flatten()(x)\n",
                "    x = keras.layers.Dense(units=256, activation=None)(x)\n",
                "    x = keras.layers.Dropout(rate=0.5)(x)\n",
                "    outputs = keras.layers.Dense(units=1, activation=tf.nn.sigmoid)(x)\n",
                "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = get_model(conv_base)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Train**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = \"04_train_feature_extraction_augmentation.keras\"\n",
                "chapter_08.train_batch(train_dataset, val_dataset, test_dataset, model, model_path, epochs, batch_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Training using fine tuning**\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Convolution base**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "conv_base = keras.applications.vgg16.VGG16(\n",
                "    weights=\"imagenet\",\n",
                "    include_top=False,\n",
                ")\n",
                "# Fine tune the last convolution block\n",
                "# Freeze all other layers\n",
                "for layer in conv_base.layers[0:-4]:\n",
                "    layer.trainable = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Model**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = get_model(conv_base)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <ins />**Train**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = \"05_train_finetuning_augmentation.keras\"\n",
                "chapter_08.train_batch(train_dataset, val_dataset, test_dataset, model, model_path, epochs, batch_size)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "projects",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
