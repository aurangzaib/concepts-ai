{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Francois Chollet (2E, 2021)**\n",
    "# **Deep Learning with Python**\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. What is deep learning?** \n",
    "   \n",
    "1. Keras to train, predict, and evaluate on mnist dataset\n",
    "\n",
    "#### **2. The mathematical building blocks of neural networks**\n",
    "1. Initialize weight and bias\n",
    "2. Find prediction\n",
    "3. Find loss\n",
    "4. Find gradient\n",
    "5. Update weight and bias\n",
    "\n",
    "#### **3. Introduction to Keras and TensorFlow**\n",
    "1. TensorFlow variable and constant\n",
    "2. First order gradient\n",
    "3. Second order gradient\n",
    "4. Linear classifier — TensorFlow\n",
    "5. Linear classifier — Keras Loss and Optimizer\n",
    "6. Linear classifier — Keras Model\n",
    "\n",
    "#### **4. Getting started with neural networks**\n",
    "1. Common Terminologies\n",
    "2. Binary Classification\n",
    "3. Multiclass Classification\n",
    "4. Non-Linear Scalar Regression\n",
    "5. Feature-wise Normalization\n",
    "6. K-Fold Cross-Validation\n",
    "\n",
    "#### **5. Fundamentals of machine learning**\n",
    "1. White Noise and Zero Channels\n",
    "2. Manifold Hypothesis\n",
    "3. Dataset Split\n",
    "4. Gradient Descent Parameters\n",
    "5. Architecture Priors\n",
    "6. Model Capacity\n",
    "7. Feature Engineering\n",
    "8. Early Stopping\n",
    "9. L1/L2 Regularization\n",
    "10. Dropout\n",
    "\n",
    "#### **6. The universal workflow of machine learning**\n",
    "1. Task Definition\n",
    "2. Model Development\n",
    "3. Model Deployment\n",
    "\n",
    "#### **7. Working with Keras: A deep dive**\n",
    "1. Module Patterns:\n",
    "   1. Keras.Sequential\n",
    "   2. Keras.Model\n",
    "   3. Subclass Keras.Model\n",
    "\n",
    "2. Mixing Module Patterns\n",
    "\n",
    "3. Builtin Training and Evaluation Loops\n",
    "   1. Mnist using Keras.Model\n",
    "   2. Custom Metrics\n",
    "   3. Builtin Callbacks\n",
    "   4. Custom Callbacks\n",
    "   5. Save and Load Model\n",
    "   6. TensorBboard\n",
    "   7. Training vs Inference\n",
    "   8. Low-level Metrics\n",
    "\n",
    "4. Builtin fit with custom train_step method\n",
    "   1. Custom train_step\n",
    "   2. Builtin fit\n",
    "\n",
    "5. Custom Training and Evaluation Loops\n",
    "   1. Tensorflow function decorator\n",
    "   2. Custom train step\n",
    "   3. Custom fit\n",
    "   4. Custom test step\n",
    "   5. Custom evaluate\n",
    "\n",
    "#### **8. Introduction to deep learning for computer vision**\n",
    "\n",
    "1. Convolution theory:\n",
    "2. Tensorflow Dataset API\n",
    "3. Explore maxpooling and padding\n",
    "4. Mnist convolution network\n",
    "5. Kaggle cats-vs-dog dataset\n",
    "6. Training from scratch:\n",
    "   1. Without augmentation\n",
    "   2. With augmentation\n",
    "7. Training using feature extraction:\n",
    "   1. Without augmentation\n",
    "   2. With augmentation\n",
    "8. Training using fine tuning:\n",
    "   1. With augmentation\n",
    "  \n",
    "#### **9.  Advanced deep learning for computer vision**\n",
    "\n",
    "#### **10. Deep learning for timeseries**\n",
    "\n",
    "#### **11. Deep learning for text**\n",
    "\n",
    "#### **12. Generative deep learning**\n",
    "\n",
    "#### **13. Best practices for the real world**\n",
    "\n",
    "#### **14. Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Most Important Research Papers (Ilya Sutskever)**\n",
    "---\n",
    "\n",
    "Ilya Sutskever to John Carmack: \"If you really learn all of these, you’ll know 90% of what matters today.\n",
    "\n",
    "1. The Annotated Transformer ( https://lnkd.in/evrqygtu)\n",
    "2. The First Law of Complexodynamics ( https://lnkd.in/eu5aucVm)\n",
    "3. The Unreasonable Effectiveness of RNNs ( https://lnkd.in/e9wht6Js)\n",
    "4. Understanding LSTM Networks ( https://lnkd.in/eY4WnawT)\n",
    "5. Recurrent Neural Network Regularization ( https://lnkd.in/ebrwzuwY)\n",
    "6. Keeping Neural Networks Simple by Minimizing the Description Length of the Weights ( https://lnkd.in/e4f4s9h6)\n",
    "7. Pointer Networks ( https://lnkd.in/e6qcSXYT)\n",
    "8. ImageNet Classification with Deep CNNs ( https://lnkd.in/etrjwGmY)\n",
    "9. Order Matters: Sequence to sequence for sets ( https://lnkd.in/eYrjEHRP)\n",
    "10. GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism ( https://lnkd.in/ezFVyhyk)\n",
    "11. Deep Residual Learning for Image Recognition ( https://lnkd.in/ejJT79DE)\n",
    "12. Multi-Scale Context Aggregation by Dilated Convolutions ( https://lnkd.in/eN-p4Hi9)\n",
    "13. Neural Quantum Chemistry ( https://lnkd.in/eChquKQi)\n",
    "14. Attention Is All You Need: Transformer ( https://lnkd.in/eakhSPXf)\n",
    "15. Neural Machine Translation by Jointly Learning to Align and Translate ( https://lnkd.in/eZfrwxDG)\n",
    "16. Identity Mappings in Deep Residual Networks ( https://lnkd.in/eVuuYTTy)\n",
    "17. A Simple NN Module for Relational Reasoning ( https://lnkd.in/e9xYieKc)\n",
    "18. Variational Lossy Autoencoder ( https://lnkd.in/e8XZrzcn)\n",
    "19. Relational RNNs ( https://lnkd.in/eEs3e_MJ)\n",
    "20. Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton (https://lnkd.in/e7V-jw8S)\n",
    "21. Neural Turing Machines ( https://lnkd.in/e3qidTvP)\n",
    "22. Deep Speech 2: End-to-End Speech Recognition in English and Mandarin ( https://lnkd.in/eYDgB9cA)\n",
    "23. Scaling Laws for Neural LMs ( https://lnkd.in/ev9s6Pz2)\n",
    "24. A Tutorial Introduction to the Minimum Description Length Principle ( https://lnkd.in/eUJtMXDU)\n",
    "25. Machine Super Intelligence Dissertation ( https://lnkd.in/ebCNq64x)\n",
    "26. PAGE 434 onwards: Kolmogorov Complexity ( https://lnkd.in/ecV-qdfV)\n",
    "27. CS231n Convolutional Neural Networks for Visual Recognition ( https://cs231n.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Common Concepts**\n",
    "---\n",
    "\n",
    "#### **Terminologies:**\n",
    "1. Sample / Input\n",
    "2. Prediction / Output\n",
    "3. Ground truth / Annotation\n",
    "4. Target\n",
    "5. Loss\n",
    "6. Classes\n",
    "7. Label\n",
    "8. Binary Classification\n",
    "9. Multiclass Classification\n",
    "10. Multilabel Classification\n",
    "11. Scalar regression\n",
    "12. Vector regression\n",
    "13. Batch\n",
    "\n",
    "#### **Datasets :**\n",
    "1. Mnist — Multiclass singlelabel classification\n",
    "2. Imdb — Binary classification\n",
    "3. Reuters — Multiclass singlelabel classification \n",
    "4. Boston — Scalar regression\n",
    "5. ImageNet — Image classification\n",
    "6. Kaggle Cats-vs-Dog — Image classification\n",
    "\n",
    "#### **Model :**\n",
    "1. VGG16\n",
    "2. Xception\n",
    "3. ResNet\n",
    "4. MobileNet\n",
    "5. EfficientNEt\n",
    "6. DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### **Layers (tf.keras.layers):**\n",
    "1. (Rank 1) Dense\n",
    "2. (Rank 2) LSTM\n",
    "3. (Rank 3) Conv1D\n",
    "4. (Rank 4) Conv2D\n",
    "\n",
    "#### **Optimizers (tf.keras.optimizers):**\n",
    "1. SGD (with / without momentum)\n",
    "2. RMSprop\n",
    "3. Adam\n",
    "4. Adagrad\n",
    "\n",
    "#### **Loss Functions (tf.keras.losses):**\n",
    "1. CategoricalCrossEntropy\n",
    "2. SparseCategoricalCrossentropy\n",
    "3. BinaryCrossEntropy\n",
    "4. MeanSquaredError (MSE)\n",
    "\n",
    "#### **Metrics (tf.keras.metrics):**\n",
    "1. CategoricalAccuracy\n",
    "2. SparseCategoricalAccuracy\n",
    "3. BinaryAccuracy\n",
    "4. MeanAbsoluteError (MAE)\n",
    "5. Precision\n",
    "6. Recall\n",
    "\n",
    "#### **K-Fold Cross-Validation**\n",
    "- When dataset is small, K-Fold allows to use whole dataset for training as well as validation\n",
    "- Training dataset is divided into K number of folds.\n",
    "- Each fold uses different part of data as validation dataset\n",
    "- In each iteration:\n",
    "  - K-1 partitions are used for training\n",
    "  - 1 partition is used for validation\n",
    "- Validation score is the average of validations of all folds\n",
    "\n",
    "#### **Training Parameters**\n",
    "- **Network Parameters:**\n",
    "  - Weight \n",
    "  - Bias\n",
    "\n",
    "- **Training Parameters:**\n",
    "  - Number of layers\n",
    "  - Learning rate\n",
    "  - Batchsize\n",
    "  - Optimizer\n",
    "  - Epoch\n",
    "\n",
    "#### **Training Priors**\n",
    "  - Dataset\n",
    "  - Model\n",
    "\n",
    "#### **Overfit and underfit**\n",
    "  - Overfit\n",
    "    - From features point-of-view:\n",
    "      - Many features\n",
    "      - Small dataset\n",
    "      - High information density\n",
    "    - From model point-of-view:\n",
    "      - Many parameters / layers (large model)\n",
    "      - Small dataset\n",
    "      - High learning capacity\n",
    "    - Given infinite data, model never overfits\n",
    "  - Underfit\n",
    "    - From features point-of-view:\n",
    "      - Few features\n",
    "      - Large dataset\n",
    "      - Low information density\n",
    "    - From model point-of-view:\n",
    "      - Few parameters / layers (small model)\n",
    "      - Large dataset\n",
    "      - Low learning capacity\n",
    "  - Recommendation:\n",
    "    - Medium information density (number of features relative to dataset size)\n",
    "    - Medium learning capacity (number of layers relative to dataset size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
