{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Francois Chollet (2E, 2021)**\n",
    "# **Deep Learning with Python**\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. What is deep learning?** \n",
    "   \n",
    "1. Keras to train, predict, and evaluate on mnist dataset\n",
    "\n",
    "#### **2. The mathematical building blocks of neural networks**\n",
    "1. Initialize weight and bias\n",
    "2. Find prediction\n",
    "3. Find loss\n",
    "4. Find gradient\n",
    "5. Update weight and bias\n",
    "\n",
    "#### **3. Introduction to Keras and TensorFlow**\n",
    "1. TensorFlow variable and constant\n",
    "2. First order gradient\n",
    "3. Second order gradient\n",
    "4. Linear classifier — TensorFlow\n",
    "5. Linear classifier — Keras Loss and Optimizer\n",
    "6. Linear classifier — Keras Model\n",
    "\n",
    "#### **4. Getting started with neural networks**\n",
    "1. Common Terminologies\n",
    "2. Binary Classification\n",
    "3. Multiclass Classification\n",
    "4. Non-Linear Scalar Regression\n",
    "5. Feature-wise Normalization\n",
    "6. K-Fold Cross-Validation\n",
    "\n",
    "#### **5. Fundamentals of machine learning**\n",
    "1. White Noise and Zero Channels\n",
    "2. Manifold Hypothesis\n",
    "3. Dataset Split\n",
    "4. Gradient Descent Parameters\n",
    "5. Architecture Priors\n",
    "6. Model Capacity\n",
    "7. Feature Engineering\n",
    "8. Early Stopping\n",
    "9. L1/L2 Regularization\n",
    "10. Dropout\n",
    "\n",
    "#### **6. The universal workflow of machine learning**\n",
    "1. Task Definition\n",
    "2. Model Development\n",
    "3. Model Deployment\n",
    "\n",
    "#### **7. Working with Keras: A deep dive**\n",
    "1. Module Patterns:\n",
    "   1. Keras.Sequential\n",
    "   2. Keras.Model\n",
    "   3. Subclass Keras.Model\n",
    "\n",
    "2. Mixing Module Patterns\n",
    "\n",
    "3. Builtin Training and Evaluation Loops\n",
    "   1. Mnist using Keras.Model\n",
    "   2. Custom Metrics\n",
    "   3. Builtin Callbacks\n",
    "   4. Custom Callbacks\n",
    "   5. Save and Load Model\n",
    "   6. TensorBboard\n",
    "   7. Training vs Inference\n",
    "   8. Low-level Metrics\n",
    "\n",
    "4. Builtin fit with custom train_step method\n",
    "   1. Custom train_step\n",
    "   2. Builtin fit\n",
    "\n",
    "5. Custom Training and Evaluation Loops\n",
    "   1. Tensorflow function decorator\n",
    "   2. Custom train step\n",
    "   3. Custom fit\n",
    "   4. Custom test step\n",
    "   5. Custom evaluate\n",
    "\n",
    "#### **8. Introduction to deep learning for computer vision**\n",
    "\n",
    "1. Convolution theory:\n",
    "2. Tensorflow Dataset API\n",
    "3. Explore maxpooling and padding\n",
    "4. Mnist convolution network\n",
    "5. Kaggle cats-vs-dog dataset\n",
    "6. Training from scratch:\n",
    "   1. Without augmentation\n",
    "   2. With augmentation\n",
    "7. Training using feature extraction:\n",
    "   1. Without augmentation\n",
    "   2. With augmentation\n",
    "8. Training using fine tuning:\n",
    "   1. With augmentation\n",
    "  \n",
    "#### **9.  Advanced deep learning for computer vision**\n",
    "\n",
    "#### **10. Deep learning for timeseries**\n",
    "\n",
    "#### **11. Deep learning for text**\n",
    "\n",
    "#### **12. Generative deep learning**\n",
    "\n",
    "#### **13. Best practices for the real world**\n",
    "\n",
    "#### **14. Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Common Concepts**\n",
    "---\n",
    "\n",
    "#### **Terminologies:**\n",
    "1. Sample / Input\n",
    "2. Prediction / Output\n",
    "3. Ground truth / Annotation\n",
    "4. Target\n",
    "5. Loss\n",
    "6. Classes\n",
    "7. Label\n",
    "8. Binary Classification\n",
    "9. Multiclass Classification\n",
    "10. Multilabel Classification\n",
    "11. Scalar regression\n",
    "12. Vector regression\n",
    "13. Batch\n",
    "\n",
    "#### **Datasets :**\n",
    "1. Mnist — Multiclass singlelabel classification\n",
    "2. Imdb — Binary classification\n",
    "3. Reuters — Multiclass singlelabel classification \n",
    "4. Boston — Scalar regression\n",
    "5. ImageNet — Image classification\n",
    "6. Kaggle Cats-vs-Dog — Image classification\n",
    "\n",
    "#### **Model :**\n",
    "1. VGG16\n",
    "2. Xception\n",
    "3. ResNet\n",
    "4. MobileNet\n",
    "5. EfficientNEt\n",
    "6. DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### **Layers (tf.keras.layers):**\n",
    "1. (Rank 1) Dense\n",
    "2. (Rank 2) LSTM\n",
    "3. (Rank 3) Conv1D\n",
    "4. (Rank 4) Conv2D\n",
    "\n",
    "#### **Optimizers (tf.keras.optimizers):**\n",
    "1. SGD (with / without momentum)\n",
    "2. RMSprop\n",
    "3. Adam\n",
    "4. Adagrad\n",
    "\n",
    "#### **Loss Functions (tf.keras.losses):**\n",
    "1. CategoricalCrossEntropy\n",
    "2. SparseCategoricalCrossentropy\n",
    "3. BinaryCrossEntropy\n",
    "4. MeanSquaredError (MSE)\n",
    "\n",
    "#### **Metrics (tf.keras.metrics):**\n",
    "1. CategoricalAccuracy\n",
    "2. SparseCategoricalAccuracy\n",
    "3. BinaryAccuracy\n",
    "4. MeanAbsoluteError (MAE)\n",
    "5. Precision\n",
    "6. Recall\n",
    "\n",
    "#### **K-Fold Cross-Validation**\n",
    "- When dataset is small, K-Fold allows to use whole dataset for training as well as validation\n",
    "- Training dataset is divided into K number of folds.\n",
    "- Each fold uses different part of data as validation dataset\n",
    "- In each iteration:\n",
    "  - K-1 partitions are used for training\n",
    "  - 1 partition is used for validation\n",
    "- Validation score is the average of validations of all folds\n",
    "\n",
    "#### **Training Parameters**\n",
    "- **Network Parameters:**\n",
    "  - Weight \n",
    "  - Bias\n",
    "\n",
    "- **Training Parameters:**\n",
    "  - Number of layers\n",
    "  - Learning rate\n",
    "  - Batchsize\n",
    "  - Optimizer\n",
    "  - Epoch\n",
    "\n",
    "#### **Training Priors**\n",
    "  - Dataset\n",
    "  - Model\n",
    "\n",
    "#### **Overfit and underfit**\n",
    "  - Overfit\n",
    "    - From features point-of-view:\n",
    "      - Many features\n",
    "      - Small dataset\n",
    "      - High information density\n",
    "    - From model point-of-view:\n",
    "      - Many parameters / layers (large model)\n",
    "      - Small dataset\n",
    "      - High learning capacity\n",
    "    - Given infinite data, model never overfits\n",
    "  - Underfit\n",
    "    - From features point-of-view:\n",
    "      - Few features\n",
    "      - Large dataset\n",
    "      - Low information density\n",
    "    - From model point-of-view:\n",
    "      - Few parameters / layers (small model)\n",
    "      - Large dataset\n",
    "      - Low learning capacity\n",
    "  - Recommendation:\n",
    "    - Medium information density (number of features relative to dataset size)\n",
    "    - Medium learning capacity (number of layers relative to dataset size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
