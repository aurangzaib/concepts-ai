{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Chapter 10**\n",
    "# **Timeseries**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed Forward Networks:**\n",
    "- Using Dense or Conv layers\n",
    "- Dense and Conv layers process each input independently â€” no memory\n",
    "- Dense layer\n",
    "  - It flattens the data removing the notion of time and order\n",
    "- Conv layer\n",
    "  - It assumes translation invariancy (i.e. properties are same irrespective of location)\n",
    "- Dense and Conv will work when the whole dataset is shown at once as an array\n",
    "\n",
    "**Recurrent Neural Networks:**\n",
    "- It uses internal loop and reuses quantities calculated in previous interation\n",
    "- Similar to residual connection\n",
    "- Long Short Time Memory (LSTM)\n",
    "  - **By saving previous outputs**, it helps preventing older information from gradually vanishing during processing\n",
    "  - It allows past information to be re-injected at later time\n",
    "  - It uses carry dataflow concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Considerations**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature-wise normalization**:\n",
    "- When input data features (e.g. XYZ) dont have same range\n",
    "- Mean centered around 0 and standard deviation around 1\n",
    "- See Chapter 04\n",
    "\n",
    "**Uniform Dropout:**\n",
    "- Same dropout mask (pattern of dropping) should be applied to all timesteps\n",
    "- Do not drop random input units (unlike in Dense and Conv layes)\n",
    "- It allows to propagate learning error through time\n",
    "- Applies to input units of layer\n",
    "- Its layer of a network\n",
    "\n",
    "**Recurrent Dropout:**\n",
    "- Uniform dropout alone does not help in regularization\n",
    "- It is used along with uniform dropout\n",
    "- Applies to recurrent units of layer\n",
    "- Not supported by CUDA\n",
    "- Its parameter of a layer\n",
    "\n",
    "**Stacking recurrent layers:**\n",
    "- When using multiple LSTM layers\n",
    "- Intermediate layers should return full output sequence rather than output at last timestep\n",
    "- keras.layers.LSTM(unit=32, recurrent_dropout=0.25, return_sequence=True)\n",
    "\n",
    "**Bidirectional recurrent network:**\n",
    "- Chronological:\n",
    "  - Recent past is more important than distant past \n",
    "  - First array element is the oldest data\n",
    "- Antichronological:\n",
    "  - Distance past is more important than recent past\n",
    "  - First array element is the newest data\n",
    "- Unidirectional LSTM: \n",
    "  - Order is important. Type of order is important (chronological)\n",
    "  - E.g. weather (recent past is more important than distant past)\n",
    "- Bidirectional LSTM:\n",
    "  - Order is important. Type of order (chronological/antichronological) is not important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
